{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 399 Final Project - Steve Kent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Goals:\n",
    "I am interested in creating new sentences based on the top POS (parts of speech) words used in gothic literature. The tool I will make can take a gothic sentence and create a new sentence with the same POS relations but new words that are commonly found in gothic books. This tool would ultimately be used to create new books. Creating a book would obviously be much more challenging but this is a start in that direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Getting top POS words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will go through our gothic books and pull out the most commonly used words for each POS. \n",
    "\n",
    "The next 3 cells are used for setting up the necessary components. Includes downloading the gothic table and nltk things. Also, I prepare a POS dictionary and setup a sentence wrangler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/stevekent/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/stevekent/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/stevekent/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gothic_table = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQqRwyE0ceZREKqhuaOw8uQguTG6Alr5kocggvAnczrWaimXE8ncR--GC0o_PyVDlb-R6Z60v-XaWm9/pub?output=csv',\n",
    "                          encoding='utf-8')\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "posDict = {}\n",
    "posDict[\"CC\"] = {} \n",
    "posDict[\"CD\"] = {}\n",
    "posDict[\"DT\"] = {}\n",
    "posDict[\"EX\"] = {}\n",
    "posDict[\"FW\"] = {}\n",
    "posDict[\"IN\"] = {}\n",
    "posDict[\"JJ\"] = {}\n",
    "posDict[\"JJR\"] = {}\n",
    "posDict[\"JJS\"] = {}\n",
    "posDict[\"LS\"] = {}\n",
    "posDict[\"MD\"] = {}\n",
    "posDict[\"NN\"] = {}\n",
    "posDict[\"NNS\"] = {}\n",
    "posDict[\"NNP\"] = {}\n",
    "posDict[\"NNPS\"] = {}\n",
    "posDict[\"PDT\"] = {}\n",
    "posDict[\"POS\"] = {}\n",
    "posDict[\"PRP\"] = {}\n",
    "posDict[\"PRP$\"] = {}\n",
    "posDict[\"RB\"] = {}\n",
    "posDict[\"RBR\"] = {}\n",
    "posDict[\"RBS\"] = {}\n",
    "posDict[\"RP\"] = {}\n",
    "posDict[\"SYM\"] = {}\n",
    "posDict[\"TO\"] = {}\n",
    "posDict[\"UH\"] = {}\n",
    "posDict[\"VB\"] = {}\n",
    "posDict[\"VBD\"] = {}\n",
    "posDict[\"VBG\"] = {}\n",
    "posDict[\"VBN\"] = {}\n",
    "posDict[\"VBP\"] = {}\n",
    "posDict[\"VBZ\"] = {}\n",
    "posDict[\"WDT\"] = {}\n",
    "posDict[\"WP\"] = {}\n",
    "posDict[\"WP$\"] = {}\n",
    "posDict[\"WRB\"] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "legals = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def sentence_wrangler(sentence, legal_chars):\n",
    "    tokes = word_punct_tokenizer.tokenize(sentence)\n",
    "    result = []\n",
    "    removed = []\n",
    "    check = 0\n",
    "    for word in tokes:\n",
    "        word = word.lower()\n",
    "        for i in word:\n",
    "            if i in legal_chars:\n",
    "                check = 1\n",
    "            else: \n",
    "                check = 0\n",
    "                break\n",
    "        if check == 1:\n",
    "            result.append(word.lower())\n",
    "        else:\n",
    "            removed.append(word.lower())\n",
    "\n",
    "    return (result, removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am going to go throught the gothic books and create the POS dictionary. First I wrangle the sentence to get words without puncuation or capital letters. Then I get the POS tag for the words in the sentence. Finally, I go through the output from pos_tag and add items to the POS dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "2797/19579 finished...\n",
      "5594/19579 finished...\n",
      "8391/19579 finished...\n",
      "11188/19579 finished...\n",
      "13985/19579 finished...\n",
      "16782/19579 finished...\n",
      "19579/19579 finished...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "count = 2797 #used for timing\n",
    "for j in range(len(gothic_table)):\n",
    "    sentence = gothic_table.loc[j, 'text']\n",
    "    author = gothic_table.loc[j, 'author']\n",
    "    words = sentence_wrangler(sentence, legals)[0]\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for i in range(len(pos)):\n",
    "        if pos[i][1] in posDict:\n",
    "            if pos[i][0] not in posDict[pos[i][1]]:\n",
    "                if author == \"EAP\":\n",
    "                    posDict[pos[i][1]][pos[i][0]] = [1, 0, 0]\n",
    "                if author == \"HPL\":\n",
    "                    posDict[pos[i][1]][pos[i][0]] = [0, 1, 0]\n",
    "                if author == \"MWS\":\n",
    "                    posDict[pos[i][1]][pos[i][0]] = [0, 0, 1]\n",
    "            else:\n",
    "                if author == \"EAP\":\n",
    "                    posDict[pos[i][1]][pos[i][0]][0] += 1\n",
    "                if author == \"HPL\":\n",
    "                    posDict[pos[i][1]][pos[i][0]][1] += 1\n",
    "                if author == \"MWS\":\n",
    "                    posDict[pos[i][1]][pos[i][0]][2] += 1\n",
    "    #timing\n",
    "    if j == 0:\n",
    "        print \"starting...\"\n",
    "    if j == count:\n",
    "        print ('%d/19579 finished...' % count)\n",
    "        count += 2797\n",
    "        if count == 19579:\n",
    "            print ('%d/19579 finished...' % count)\n",
    "            print \"done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will create a function called \"most_important\" that takes an author, a POS and the POS dictionary and returns the most important words for the author for that POS. This function will use the calculate_importance function from midterm 1 to determine which author claims each word most important to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_importance(counts_list):\n",
    "    count = 0.0\n",
    "    dont_use_word = 0.0\n",
    "    results = []\n",
    "    for i in counts_list:\n",
    "        count += 1\n",
    "        if i == 0:\n",
    "            dont_use_word += 1\n",
    "    for i in counts_list:\n",
    "        tf = i\n",
    "        idf = math.log10(count/(count-dont_use_word))\n",
    "        results.append(tf*idf)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important(relation, author, dictionary):\n",
    "    output = []\n",
    "    for word in dictionary[relation].items():\n",
    "        result = calculate_importance(word[1])\n",
    "        if author == \"EAP\":\n",
    "            if result[0] > result[2] and result[0] > result[1]:\n",
    "                output.append(word[0])\n",
    "        if author == \"HPL\":\n",
    "            if result[1] > result[2] and result[1] > result[0]:\n",
    "                output.append(word[0])\n",
    "        if author == \"MWS\":\n",
    "            if result[2] > result[0] and result[2] > result[1]:\n",
    "                output.append(word[0])\n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'hath', u'saved', u'crooned', u'sputtered', u'patched', u'characterised', u'slippered', u'wooded', u'sunk', u'festered', u'favoured', u'beaten', u'pawed', u'sidled', u'stared', u'ploughed', u'represented', u'shriek', u'buckled', u'proportioned', u'oozed', u'gloated', u'postponed', u'reckoned', u'charred', u'uncased', u'menaced', u'hole', u'moulded', u'give', u'flung', u'poked', u'kranon', u'plied', u'infested', u'dreamed', u'rewarded', u'fit', u'diseased', u'dovetailed', u'disembodied', u'swilled', u'towered', u'laboured', u'mangled', u'elwood', u'hanged', u'waddled', u'haired', u'washed', u'disputed', u'conflicted', u'eared', u'ritual', u'delved', u'chopped', u'twitched', u'dwelt', u'masted', u'courted', u'top', u'twisted', u'blamed', u'stuffed', u'excelled', u'injected', u'lunged', u'garlanded', u'mercy', u'matted', u'scraped', u'soaked', u'cleft', u'degenerated', u'glimpsed', u'reverberated', u'circled', u'whar', u'episode', u'inferred', u'enshrined', u'alight', u'flouted', u'larnt', u'chewed', u'remounted', u'paned', u'squirmed', u'wizened', u'congealed', u'prefaced', u'exchanged', u'lunched', u'boarded', u'flagged', u'hasn', u'demented', u'fanged', u'reported', u'smuggled', u'beauclerk', u'repelled', u'dast', u'pecked', u'tolled', u'harboured', u'bulged', u'wonted', u'climbed', u'dumped', u'curved', u'illumed', u'heeded', u'bearded', u'boiled', u'progressed', u'greeted', u'kin', u'stiffened', u'centred', u'prescribed', u'whereby', u'paralysed', u'ragged', u'smoked', u'portrayed', u'fared', u'spectacled', u'transcribed', u'expelled', u'posed', u'aran', u'lapsed', u'cooled', u'lurked', u'clawed', u'ruffled', u'bottom', u'leered', u'summoned', u'dispelled', u'shrivelled', u'translated', u'flourished', u'coiled', u'lacked', u'hooved', u'balanced', u'minted', u'weathered', u'sunlight', u'drowsed', u'perfumed', u'ruined', u'fattened', u'guessed', u'fashioned', u'dotted', u'whereat', u'trotted', u'unchallenged', u'necessitated', u'managed', u'hissed', u'switched', u'dimensioned', u'bordered', u'donned', u'registered', u'hindered', u'sunup', u'reaffirmed', u'sordid', u'encrusted', u'shared', u'unobserved', u'grinned', u'diagnosed', u'plotted', u'sighted', u'shouldered', u'lessened', u'scratched', u'walled', u'seeped', u'decked', u'deformed', u'deceased', u'mauled', u'dropped', u'nevil', u'recited', u'onct', u'secured', u'revolted', u'undertook', u'clattered', u'disgusted', u'crashed', u'piloted', u'whenever', u'darkened', u'strowed', u'stayed', u'cold', u'blooded', u'rumoured', u'telled', u'rattled', u'subordinated', u'matched', u'shouted', u'peered', u'ef', u'cinctured', u'hemmed', u'clanked', u'ivied', u'browed', u'spied', u'scarred', u'plodded', u'couldn', u'comprised', u'enthralled', u'rented', u'effaced', u'alfred', u'hut', u'eyed', u'slashed', u'surged', u'damned', u'cowed', u'tasted', u'hooded', u'shocked', u'skinned', u'deciphered', u'printed', u'inserted', u'wreathed', u'reiterated', u'shou', u'pencilled', u'babbled', u'lettered', u'chiselled', u'mowry', u'enfeebled', u'embittered', u'wood', u'cautioned', u'guarded', u'windowed', u'unfastened', u'seared', u'snorted', u'worried', u'predicted', u'rifted', u'digressed', u'anchored', u'aroused', u'fascinated', u'boy', u'radiated', u'moaned', u'whipped', u'shivered', u'loped', u'tuk', u'traded', u'arcaded', u'cornered', u'muttered', u'unopened', u'zigzagged', u'crawford', u'implored', u'sound', u'squeezed', u'tarnished', u'acclaimed', u'malformed', u'bareheaded', u'verified', u'granted', u'edged', u'squatted', u'muffled', u'statue', u'despatched', u'smelt', u'relied', u'fidgeted', u'lumbered', u'steamed', u'guided', u'kilt', u'cease', u'snarled', u'captured', u'palled', u'shingled', u'jarred', u'engulfed', u'sucked', u'gestured', u'sheared', u'inted', u't', u'endorsed', u'classified', u'alternated', u'reversed', u'welcome', u'drums', u'baffled', u'mixed', u'perturbed', u'speculated', u'wade', u'cared', u'rumbled', u'cato', u'hed', u'sealed', u'heh', u'bristled', u'beyont', u'swathed', u'pitched', u'panelled', u'billowed', u'railed', u'fright', u'angled', u'companioned', u'scanned', u'trickled', u'roomed', u'stumbled', u'administered', u'biassed', u'stilted', u'twinkled', u'overturned', u'moonlight', u'lapped', u'forged', u'espied', u'wakened', u'evicted', u'shattered', u'puzzled', u'bubbled', u'coloured', u'hieroglyphed', u'robed', u'veined', u'heaved', u'sang', u'deftness', u'undulated', u'slavered', u'rehearsed', u'faltered', u'whirled', u'chipped', u'sunburned', u'elongated', u'brimmed', u'apprised', u'trained', u'armed', u'spawned', u'barked', u'waxed', u'tested', u'mounted', u'helped', u'intercepted', u'debated', u'lingered', u'accept', u'pushed', u'disliked', u'courtesied', u'unexpected', u'thickened', u'blacker', u'telephoned', u'deserted', u'developed', u'brig', u'unpolluted', u'detected', u'clapboarded', u'focussed', u'hunted', u'groped', u'coursed', u'dulled', u'fluttered', u'malone', u'organised', u'shamefaced', u'missed', u'crouched', u'housed', u'combed', u'outfitted', u'corrugated', u'emblazoned', u'prophesied', u'numbed', u'symbolised', u'gurgled', u'silhouetted', u'pitted', u'shifted', u'draped', u'peaked', u'imperiled', u'gnawed', u'i', u'insulted', u'gibbered', u'stickiness', u'cleaned', u'creased', u'aged', u'goldsmith', u'lid', u'flared', u'cropped', u'extracted', u'rowed', u'penetrated', u'patterned', u'charged', u'stalked', u'celebrated', u'gold', u'perched', u'gilded', u'grazed', u'hit', u'slithered', u'hid', u'gloved', u'spliced', u'grey', u'sidetracked', u'pocketed', u'burney', u'surmounted', u'feasted', u'spoiled', u'incinerated', u'exhumed', u'grounded', u'journeyed', u'intermarried', u'blazed', u'afar', u'swirled', u'liked', u'danced', u'whatever', u'caked', u'steepled', u'woke', u'raced', u'folklore', u'fur', u'canvassed', u'wriggled', u'overlooked', u'peopled', u'used', u'flooded', u'knowed', u'wedged', u'sneered', u'conjured', u'cud', u'applauded', u'ranted', u'awaked', u'clutched', u'd', u'untended', u'rhythmed', u'shimmered', u'jolted', u'minced', u'hinted', u'scuttled', u'dilapidated', u'flashed', u'esquimaux', u'obscured', u'moored', u'entailed', u'straightened', u'misplaced', u'fallin', u'sprawled', u'amidst', u'creaked', u'worshipped', u'signified', u'faound', u'pounded', u'collapsed', u'hoped', u'fishy', u'filed', u'sullied', u'yelled', u'consulted', u'grouped', u'sided', u'pretended', u'worsted', u'implied', u'dogged', u'amplified', u'detoured', u'neighed', u'flickered', u'betwixt', u'carted', u'evoked', u'rotted', u'piped', u'slabs', u'tremor', u'alhazred', u'veered', u'climb', u'shoved', u'reanimated', u'scaled', u'snapped', u'galloped', u'carrington', u'hydra', u'fer', u'assailed', u'ashamed', u'slid', u'bolstered', u'paled', u'mumbled', u'battered', u'bothered', u'halted', u'shuffled', u'light', u'overmastered', u'lined', u'rusted', u'whimpered', u'flopped', u'hankered', u'brood', u'll', u'shewed', u'ceiled', u'emphasised', u'flee', u'warned', u'transmitted', u'steadied', u'aggravated', u'shambled', u'marvelled', u'loomed', u'timed', u'splintered', u'tacked', u'manuscript', u'braced', u'reigned', u'ancient', u'visaged', u'afeared', u'ben', u'reacted', u'humped', u'deterred', u'tonight', u'waked', u'yearned', u'atoned', u'viewed', u'afraid', u'desarved', u'camped', u'bred', u'satan', u'abutted', u'fumbled', u'meant', u'striped', u'whilst', u'ministered', u'fixed', u'racked', u'allured', u'surmised', u'welled', u'encamped', u'blinked', u'wilbur', u'unbuckled', u'clogged', u'allaowed', u'ransomed', u'pounced', u'erased', u'quit', u'faced', u'stationed', u'gouged', u'corresponded', u'sixty', u'headlight', u'merciful', u'discouraged', u'builded', u'floored', u'glowed', u'realised', u'paul', u'lured', u'devised', u'crawled', u'honeycombed', u'rebelled', u'library', u'hair', u'whined', u'leaded', u'coated', u'analysed', u'obed', u'unhinged', u'flattened', u'tomb', u'yanked', u'glistened', u'noticed', u'coke', u'blowed', u'signed', u'quarried', u'nursed', u'crushed', u'curtained', u'permeated', u'searched']\n"
     ]
    }
   ],
   "source": [
    "out = most_important('VBD', 'HPL', posDict)\n",
    "print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I created a function that will go through the POS dict and find the most common words for all authors instead of just one. Also, I demonstrate how this could be used using the random.choice function on the returned list. This will give me a random top word to be used in creating a new gothic sentence. This function will not be used further but is an alternate route to picking out words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'small', [199, 289, 135])\n"
     ]
    }
   ],
   "source": [
    "import collections as c \n",
    "import random as r\n",
    "\n",
    "def getMostCommon(relation, sourceDict): \n",
    "    for subdict in sourceDict.items(): \n",
    "        if subdict[0] == relation: \n",
    "            out = c.Counter(subdict[1]) \n",
    "            return out.most_common()[:50]\n",
    "        \n",
    "result = getMostCommon('JJ', posDict)\n",
    "print (r.choice(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Creating Partial Sentence Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool will act sort of like a Mad Libs machine for gothic literature. I am going to give user option to take either a random word from what I've done above or take a random synonym. The tool must also accept partially completed sentences so first I will need to update sentence wrangler so that it can handle partially completed sentences. \n",
    "\n",
    "Sentences that are incomplete will need to follow this general format:\n",
    "\n",
    "\"Victor Frankenstein VBZ the creature in his laboratory\"\n",
    "\n",
    "The incomplete words will need to have their POS as a place holder. This also allows for the entire sentence to be wildcards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Victor', 'Frankenstein', 'builds', 'the', 'NN', 'in', 'his', 'laboratory']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Victor Frankenstein builds the NN in his laboratory\"\n",
    "\n",
    "legals2 = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "#Similar to original sentence wrangler, just ignores capitalization.\n",
    "def sentence_wrangler2(sentence, legal_chars):\n",
    "    tokes = word_punct_tokenizer.tokenize(sentence)\n",
    "    result = []\n",
    "    removed = []\n",
    "    check = 0\n",
    "    for word in tokes:\n",
    "        for i in word:\n",
    "            if i in legal_chars:\n",
    "                check = 1\n",
    "            else: \n",
    "                check = 0\n",
    "                break\n",
    "        if check == 1:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            removed.append(word)\n",
    "\n",
    "    return (result, removed)\n",
    "            \n",
    "words = sentence_wrangler2(test_sentence, legals2)[0]\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the actual tool. Note that each time the function is called, a new word will be chosen and the sentence will be different. This tool isn't perfect because some words don't work in the original sentence but it is pretty interesting and at times amusing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_sentence(sentence, author):\n",
    "    words = sentence_wrangler2(sentence, legals2)[0]\n",
    "    new_sentence = []\n",
    "    return_string = \"\"\n",
    "    for word in words:\n",
    "        if word in posDict:\n",
    "            most = most_important(word, author, posDict) #using most_important\n",
    "            replace = r.choice(most)\n",
    "            new_sentence.append(replace)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    for word in new_sentence:\n",
    "        return_string += word + \" \"\n",
    "    return return_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victor Frankenstein builds the mime in his laboratory \n"
     ]
    }
   ],
   "source": [
    "print partial_sentence(test_sentence, 'MWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Complete Sentence to New Sentence Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final tool takes a sentence, extracts the POS relations and building a new sentence. The new sentence can be built with either gothic words or synonyms for each word. This tool cannot accept wildcards and will need to be a complete sentence. The gothic flavor argument is where the user gets the option between a gothic word or a synonym (True for \"yes gothic\", False for \"no, I want synonyms\"). The POStoChange argument will allow user to change only one type of POS in the sentence (for example, change just the nouns). If POStoChange is set to an empty string/is unspecified, then the function will change all words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells are helper functions. Get Syns returns a list of synonym words. Show POS simply outputs the POS for each word in a sentence so the user can choose which POS they want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_syns(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lem in syn.lemmas():\n",
    "            synonyms.append(lem.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pos(sentence):\n",
    "    words = sentence_wrangler2(sentence, legals2)[0]\n",
    "    pos = nltk.pos_tag(words)\n",
    "    print pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence2 = \"Victor Frankenstein builds the creature in his laboratory\"\n",
    "test_sentence3 = \"the monster kills Frankenstein's best friend Henry Clerva.\"\n",
    "\n",
    "def completely_new_sentence(sentence, gothic_flavor, POStoChange):\n",
    "    words = sentence_wrangler2(sentence, legals2)[0]\n",
    "    pos = nltk.pos_tag(words)\n",
    "    new_sentence = []\n",
    "    return_string = \"\"\n",
    "    if gothic_flavor: \n",
    "        for tup in pos:\n",
    "            if len(POStoChange) > 1:\n",
    "                if tup[1] == POStoChange:\n",
    "                    most = most_important(tup[1], author, posDict) #using most_important\n",
    "                    replace = r.choice(most)\n",
    "                    new_sentence.append(replace)\n",
    "                else:\n",
    "                    new_sentence.append(tup[0])\n",
    "            else:\n",
    "                most = most_important(tup[1], author, posDict) #using most_important\n",
    "                replace = r.choice(most)\n",
    "                new_sentence.append(replace)\n",
    "    else:\n",
    "        for tup in pos:\n",
    "            if len(POStoChange) > 1:\n",
    "                if tup[1] == POStoChange:\n",
    "                    syn = get_syns(tup[0])\n",
    "                    if syn:\n",
    "                        replace = r.choice(syn)\n",
    "                        new_sentence.append(replace)\n",
    "                    else:\n",
    "                        new_sentence.append(tup[0])\n",
    "                else:\n",
    "                    new_sentence.append(tup[0])\n",
    "            else:\n",
    "                most = most_important(tup[1], author, posDict) #using most_important\n",
    "                replace = r.choice(most)\n",
    "                new_sentence.append(replace)\n",
    "    for word in new_sentence:\n",
    "        return_string += word + \" \"\n",
    "    return return_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Victor', 'NNP'), ('Frankenstein', 'NNP'), ('builds', 'VBZ'), ('the', 'DT'), ('creature', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('laboratory', 'NN')]\n",
      "=====================================================\n",
      "xiith yog enjoys azathoth greaser arkham hiram postumius \n"
     ]
    }
   ],
   "source": [
    "show_pos(test_sentence2)\n",
    "print \"=====================================================\"\n",
    "print completely_new_sentence(test_sentence2, False, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these two tools are pretty interesting and act as an effective MadLib-type machine. Adding specifc words from specific authors is a good step towards writing a new book in the sytle of one author. That would be the ultimate goal for these tools even if that goal is impossible. I think the next logical step would be to look at commonly used POS patterns by certain authors and then creating those kinds of sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
