{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "_cell_guid": "a3cb0ee3-7bca-4b2b-8a27-be198d18818e",
    "_uuid": "075ab0f3fc310e293828b3681f1d80642f88c106"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>\n",
    "<center>\n",
    "Module 5 - Fun with document vectors\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I want to continue to explore using the values in bag-of-words to build vectors. The general idea is that we will generate a vector for EAP, a vector for HPL and a vector for MWS. How do we get these vectors? Simple. We take a column from bag-of-words. Before going further, let's read in bag-of-wordsÂ fromn week 4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'aaem', [1, 0, 0]),\n",
       " (u'ab', [1, 0, 0]),\n",
       " (u'aback', [2, 0, 0]),\n",
       " (u'abaft', [0, 0, 1]),\n",
       " (u'abandon', [7, 3, 1]),\n",
       " (u'abandoned', [11, 13, 5]),\n",
       " (u'abandoning', [2, 1, 0]),\n",
       " (u'abandonment', [2, 0, 3]),\n",
       " (u'abaout', [0, 24, 0]),\n",
       " (u'abased', [1, 0, 0])]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I am using dropbox so got the url to my file. If you have on local drive, then use file reading code\n",
    "\n",
    "import json\n",
    "bag_of_words = json.load(open(\"bag_of_words.txt\"))\n",
    "sorted_items = sorted(bag_of_words.items())  # need to sort to make sure vectors align\n",
    "sorted_items[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 1\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Let's write a better version of sentence_wrangler. What I noticed this week when going through new books is that I was letting some strange words through. For instance, my sentence_wrangler from last week lets numbers through. And it also lets byte codes through. I think a better design would be to switch from a blacklist (define chars don't want) to a whitelist (define chars that are ok). Change the 3rd argument to the set of legal characters you allow.\n",
    "<p>\n",
    "If you want to be fancy, be my guest. Use the 3rd argument to pass in an re pattern that needs to match against each word. Much more elegant.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sentence_wrangler(sentence, swords, legal_chars):\n",
    "    tokes = word_punct_tokenizer.tokenize(sentence)\n",
    "    result = []\n",
    "    removed = []\n",
    "    check = 0\n",
    "    for word in tokes:\n",
    "        word = word.replace(\"_\", \"\")\n",
    "        if word.lower() not in swords:\n",
    "            for i in word.lower():\n",
    "                if i in legal_chars:\n",
    "                    check = 1\n",
    "                else: \n",
    "                    check = 0\n",
    "            if check == 1:\n",
    "                result.append(word.lower())\n",
    "            else:\n",
    "                removed.append(word.lower())\n",
    "        else:\n",
    "            removed.append(word.lower())\n",
    "    return (result, removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is my whitelist - re pattern would be better. Extra credit if you do it\n",
    "\n",
    "#legals = r'...'\n",
    "\n",
    "legals = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Some other odds and ends. We will need cosine_similarity from prior module, stop words and tokenizer.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    AB = 0.0\n",
    "    A = 0.0\n",
    "    B = 0.0\n",
    "    for i in range(len(v1)):\n",
    "        AB += (v1[i] * v2[i])\n",
    "        A += (v1[i]**2)\n",
    "        B += (v2[i]**2)  \n",
    "    return AB/((A*B)**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Ok, let's get to it\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "What I want to know is how \"close\" 2 books are to each other. I'll build a word-count vector for each book. And then take the cosine similarity. I'll give you a start.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item in sorted_items: (word, (eap_val, hpl_val, mws_val))\n",
    "sorted_items = []\n",
    "\n",
    "for word in sorted(bag_of_words):\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(tuple(bag_of_words[word]))\n",
    "    sorted_items.append(tuple(temp))  \n",
    "\n",
    "eap_vector = [pair[1][0] for pair in sorted_items]\n",
    "hpl_vector = [pair[1][1] for pair in sorted_items]\n",
    "mws_vector = [pair[1][2] for pair in sorted_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7487332567707315"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_hpl = cosine_similarity(eap_vector, hpl_vector)\n",
    "eap_hpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Is that close?\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "The range of the cosine similarity for us is 0..1. Does that make .75 high? It is hard to answer this without having the values for other book combinations. I would say it is high enough to warrant a further look if I was searching for plagiarism. Let's check out some other combos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 2\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Go ahead and do the other 2 comparisons.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7462905158479859"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_mws = cosine_similarity(eap_vector, mws_vector)\n",
    "eap_mws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371094246610521"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mws_hpl = cosine_similarity(mws_vector, hpl_vector)\n",
    "mws_hpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Kind of interesting\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "All 3 have roughly same similarity score. I would expect that given they are all gothic novels. Do you think we are catching the gothic/horror genre in our vectors through use of words?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 3\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Let's test our conjecture that we are capturing something about gothicness. Let's compare the 3 against Huckleberry Finn by Mark Twain. My gut feeling is that this should not be high on gothic scale. Your goal is to build a huck_vector that you can compare against our existing vectors. Here is what you need to do:\n",
    "<p>\n",
    "<ol>\n",
    "<li>Initialize a huck_dict that has same keys as bag_of_words and each key's value is a count of that word in the Huck Finn book.\n",
    "<li>Find an online version of Huck Finn. Hint: Project Gutenberg is a great source.\n",
    "<li>Figure out how to read the book in and to break the book into sentences.\n",
    "<li>Pass each sentence through sentence_wrangler to get words.\n",
    "<li>For each word, increase the count for huck_dict[word], but only if word is in bag_of_words. If the word is not in bag_of_words, add it to the list huck_left_out.\n",
    "</ol>\n",
    "<p>\n",
    "Check your results against mine.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "url = \"http://www.gutenberg.org/files/76/76-0.txt\"\n",
    "f = urllib2.urlopen(url)\n",
    "\n",
    "\n",
    "alltext = \"\"\n",
    "\n",
    "#count = 0\n",
    "for line in f.readlines():\n",
    "    #count += 1\n",
    "    #if count > 23 and count < 11976:\n",
    "    alltext += line\n",
    "\n",
    "allsentences = word_punct_tokenizer.tokenize(alltext.decode('utf-8'))\n",
    "\n",
    "all_huck_words = bag_of_words.copy()\n",
    "huck_left_out = []\n",
    "for word in all_huck_words:\n",
    "    all_huck_words[word] = 0\n",
    "for sentence in allsentences:\n",
    "    wrangled = sentence_wrangler(sentence, swords, legals)\n",
    "    for word in wrangled[0]:\n",
    "        if word in bag_of_words:\n",
    "            if word not in all_huck_words:\n",
    "                all_huck_words[word] = 1\n",
    "            else:\n",
    "                all_huck_words[word] += 1\n",
    "        else: \n",
    "            huck_left_out.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24944"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_huck_words)  # we expect this to be 24944, the len of bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'aaem', 0),\n",
       " (u'ab', 3),\n",
       " (u'aback', 0),\n",
       " (u'abaft', 0),\n",
       " (u'abandon', 0),\n",
       " (u'abandoned', 0),\n",
       " (u'abandoning', 0),\n",
       " (u'abandonment', 0),\n",
       " (u'abaout', 0),\n",
       " (u'abased', 0)]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_huck_words.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1927"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(huck_left_out))  #number of unique words left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abner',\n",
       " u'abolitionist',\n",
       " u'abram',\n",
       " u'abusing',\n",
       " u'accessed',\n",
       " u'ache',\n",
       " u'actin',\n",
       " u'actuly',\n",
       " u'additions',\n",
       " u'addled']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(huck_left_out)), reverse=False)[:10]  #first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'zip',\n",
       " u'yuther',\n",
       " u'yow',\n",
       " u'yourn',\n",
       " u'yo',\n",
       " u'yit',\n",
       " u'yistiddy',\n",
       " u'yisterday',\n",
       " u'yirls',\n",
       " u'yers']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(huck_left_out)), reverse=True)[:10]  #last 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "A note about these left out words\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I am keeping bag_of_words static for simplicity. But in reality, it is a growing thing. We should really add these left out words into bag_of_words and zero them out for eap, hpl and mws. As it is, we are kind of playing by gothic rules, ony using the words we saw in gothic authors. What would happen if we expanded bag_of_words to include all the new words we see in each new book? Would that move us closer or farther away from similarity with the gothic authors?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 4\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Build the huck_vector and compare with other 3. Remember to sort items so vectors align.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "huck_sorted_items = []\n",
    "\n",
    "for word in sorted(all_huck_words):\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(all_huck_words[word])\n",
    "    huck_sorted_items.append(tuple(temp)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "huck_vector = [pair[1] for pair in huck_sorted_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huck_vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5220519130559351"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_huck = cosine_similarity(eap_vector, huck_vector)\n",
    "eap_huck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575496270052048"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpl_huck = cosine_similarity(hpl_vector, huck_vector)\n",
    "hpl_huck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48183022170429957"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mws_huck = cosine_similarity(mws_vector, huck_vector)\n",
    "mws_huck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Huck Finn is definitely less similar. Closest to Lovecraft. Hmmmmm. They were writing at roughly the same time period.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 4\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Let's try one my literary colleague tells me is the antithesis of gothic: Oliver Twist by Charles Dickens.\n",
    "<p>\n",
    "Same routine as Huckleberry Finn. Find it, bag it, vectorize it, cosine it with other 4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "url = \"http://www.gutenberg.org/cache/epub/730/pg730.txt\"\n",
    "f = urllib2.urlopen(url)\n",
    "\n",
    "\n",
    "alltext = \"\"\n",
    "\n",
    "for line in f.readlines():\n",
    "    alltext += line\n",
    "\n",
    "allsentences = word_punct_tokenizer.tokenize(alltext.decode('utf-8'))\n",
    "\n",
    "all_twist_words = bag_of_words.copy()\n",
    "twist_left_out = []\n",
    "for word in all_twist_words:\n",
    "    all_twist_words[word] = 0\n",
    "for sentence in allsentences:\n",
    "    wrangled = sentence_wrangler(sentence, swords, legals)\n",
    "    for word in wrangled[0]:\n",
    "        if word in bag_of_words:\n",
    "            if word not in all_twist_words:\n",
    "                all_twist_words[word] = 1\n",
    "            else:\n",
    "                all_twist_words[word] += 1\n",
    "        else: \n",
    "            twist_left_out.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24944"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_twist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'aaem', 0),\n",
       " (u'ab', 0),\n",
       " (u'aback', 0),\n",
       " (u'abaft', 0),\n",
       " (u'abandon', 1),\n",
       " (u'abandoned', 1),\n",
       " (u'abandoning', 0),\n",
       " (u'abandonment', 0),\n",
       " (u'abaout', 0),\n",
       " (u'abased', 0)]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twist_sorted_items = []\n",
    "\n",
    "for word in sorted(all_twist_words):\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(all_twist_words[word])\n",
    "    twist_sorted_items.append(tuple(temp)) \n",
    "twist_sorted_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(twist_left_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abase',\n",
       " u'ablutions',\n",
       " u'abound',\n",
       " u'absenting',\n",
       " u'abstractedly',\n",
       " u'abuts',\n",
       " u'acause',\n",
       " u'acceded',\n",
       " u'acceding',\n",
       " u'accessed']"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(twist_left_out)), reverse=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'zip',\n",
       " u'younker',\n",
       " u'yokel',\n",
       " u'yerself',\n",
       " u'xxxviii',\n",
       " u'xxxvii',\n",
       " u'xxxvi',\n",
       " u'xxxv',\n",
       " u'xxxix',\n",
       " u'xxxiv']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(twist_left_out)), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Dang\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Looks like sentence_wrangler is letting through preface page numbers, e.g., xxxv. If we know we are dealing with books, I suppose we could write a special sentence_wrangler that knows about the weird things we will see in books and throw them out. I kind of like the idea of having a library of sentence wranglers that are tuned to specific styles of text. Then you can choose the one that makes the most sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Same left outs?\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I wonder how much an overlap there is between words being left out of Huck Finn and words left out of Oliver Twist.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1693"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection = set(twist_left_out).difference(set(huck_left_out))\n",
    "len(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "A pretty big overlap.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Ok, back to the problem\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Build the twist_vector and compare with other 4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "twist_vector = [pair[1] for pair in twist_sorted_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6556003623577531"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_twist = cosine_similarity(eap_vector, twist_vector)\n",
    "eap_twist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5937931706997982"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpl_twist = cosine_similarity(hpl_vector, twist_vector)\n",
    "hpl_twist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5688023249478481"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mws_twist = cosine_similarity(mws_vector, twist_vector)\n",
    "mws_twist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5709846751186273"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huck_twist = cosine_similarity(huck_vector, twist_vector)\n",
    "huck_twist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Poe is winner this time\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Wonder if Poe and Dickens knew each other. They were writing at roughly the same time. Maybe we are picking up on the language and jargon of the time?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 5\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I'm putting down a challenge. Find a book that has a cosine similarity value of below .5 for all 3 gothic authors. I was able to get below .4! You will get a shout-out if you can beat me.\n",
    "<p>\n",
    "To make exploration easier, I packaged up the code to produce the 3 values into a single function. For each book I explored, I saved it in my dropbox account and then got the url to it. That is what I passed into my function. You could do something similar with Google docs. Or change the url to a file path.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_book(url, bag, swords, legals):\n",
    "    f = urllib2.urlopen(url)\n",
    "    alltext = \"\"\n",
    "\n",
    "    for line in f.readlines():\n",
    "        alltext += line\n",
    "\n",
    "    allsentences = word_punct_tokenizer.tokenize(alltext.decode('utf-8'))\n",
    "\n",
    "    all_words = bag.copy()\n",
    "    for word in all_words:\n",
    "        all_words[word] = 0\n",
    "    for sentence in allsentences:\n",
    "        wrangled = sentence_wrangler(sentence, swords, legals)\n",
    "        for word in wrangled[0]:\n",
    "            if word in bag:\n",
    "                if word not in all_words:\n",
    "                    all_words[word] = 1\n",
    "                else:\n",
    "                    all_words[word] += 1\n",
    "                \n",
    "    sorted_items = []\n",
    "\n",
    "    for word in sorted(all_words):\n",
    "        temp = []\n",
    "        temp.append(word)\n",
    "        temp.append(all_words[word])\n",
    "        sorted_items.append(tuple(temp)) \n",
    "        \n",
    "    x_vector = [pair[1] for pair in sorted_items]\n",
    "    \n",
    "    eap_x = cosine_similarity(eap_vector, x_vector)\n",
    "    hpl_x = cosine_similarity(hpl_vector, x_vector)\n",
    "    mws_x = cosine_similarity(mws_vector, x_vector)\n",
    "    \n",
    "    return (eap_x, hpl_x, mws_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6556003623577531, 0.594146408238699, 0.5691251809493426)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to make sure get same values as by hand above\n",
    "check_book(\"http://www.gutenberg.org/cache/epub/730/pg730.txt\", bag_of_words, swords, legals)  # twist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24827214023748173, 0.24123167807551535, 0.2312175265968621)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_book(\"http://www.gutenberg.org/files/57141/57141-0.txt\", bag_of_words, swords, legals)  # close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5598991946885765, 0.5489741103371224, 0.49926869027946075)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_book(\"http://www.gutenberg.org/files/57136/57136-0.txt\", bag_of_words, swords, legals)  # my winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Closing note\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I'm still interested in using these word-frequency vectors to see what we can do. Next week we will take a look at another way to reason with words, i.e. word co-occurrence matrices.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
