{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a3cb0ee3-7bca-4b2b-8a27-be198d18818e",
    "_uuid": "075ab0f3fc310e293828b3681f1d80642f88c106"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>\n",
    "<center>\n",
    "Module 6 - from bag to matrix\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "This week I want to head in a slightly different direction than the bag of words approach. I want you to consider (and program) a co-occurrence matrix (comat). The comat can lead toward some very cool NLP methods.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Here's the idea\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "With bag of words, we threw away context information. If I look up bag_of_words['fawn'], I can see how many times fawn appeared but I don't know anything about what context it appeared in. The context I will be interested to start with are the two words on either side of fawn, i.e., one word on left and one word on right. If fawn is at beginning or end of sentence, then we will only count one word.\n",
    "<p>\n",
    "To get this new information about context, I will have to go through all the sentences again, and for each word I will update what word is on either side of it. Let me try a small example. Pretend we only have 3 sentences.\n",
    "<p>\n",
    "<pre>\n",
    "I love programming. I love Math. I tolerate Biology.\n",
    "</pre>\n",
    "<p>\n",
    "Check out the comat I would build from these sentences.\n",
    "<p>\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*1p0geczj9KbJvwYi25B2Jg.png'>\n",
    "<p>\n",
    "You should notice that the row names and column names are symmetric. They are made up of the unique words in all the sentences. You should also note that sentences matter. For instance, if you remove the periods, then you would get `Math` coming right before `I`. So should be an entry for `[Math, I]` of `1`. But it is zero. We do not cross sentence boundaries to check for co-occurrences.\n",
    "<p>\n",
    "Also note that we are looking at either side of a word for co-occurrences. Hence, `I` shows up with `love` twice and `love` shows up with `I` twice.\n",
    "<p>\n",
    "Finally note that the above matrix is counting a period as a word. I think that is interesting, using punctuation instead of throwing it out, but I did not do it. To match my results, use your sentence_wrangler, which throws out punctuation among other things.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "There is one parameter\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "The parameter for a comat is how far from the target word we check. In the example above, the parameter value is 1 - check 1 word on either side. But I could set it to a larger integer. If I make it big enough, I'll likely get most of the sentence as the context.\n",
    "<p>\n",
    "Reminder: parameters like this in data science are often called hyper-parameters. The \"K\" in KNN is a hyper-parameter.\n",
    "<p>\n",
    "I won't ask you to parameterize your algorithm for the main assignment: you can assume it is a constant 1. If anyone needs make-up points for past homework, I'll give you extra credit points if you can parameterize your algorithm to use a parameter. So look 2 words out or 3 words out, etc. The larger the parameter, the more context you are picking up.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Doing things a little differently\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Normally I would guide you through main steps and have you fill in pieces. I am going to cut you loose for this assignment. You can solve the problem in any way you want as long as you match my results.\n",
    "<p>\n",
    "What you will need to get started:\n",
    "<ul>\n",
    "<li>gothic_table\n",
    "<li>sentence_wrangler plus what it needs as params.\n",
    "<li>cosine_similarity\n",
    "</ul>\n",
    "<p>\n",
    "I hope you have inferred from the sample comat above that your comat will be N by N where N = 24834. Yikes. Every unique word is a row/column label. This may take a fairly big chunk of your laptop's virtual memory. Here is a piece of code that you might find useful. It tells you what percentage of memory you have remaining.\n",
    "<pre>\n",
    "<code>\n",
    "import psutil\n",
    "print(psutil.virtual_memory())  # percent is what is remaining\n",
    "</code>\n",
    "</pre>\n",
    "<p>\n",
    "I was ok on my laptop with 16GB memory. If you run into trouble on your machine, see me.\n",
    "<p>\n",
    "I also took time to compute various things. Did I mention we are now working on a `24834x24834 matix`? Things take time. On my slow machine, worst I got was 9 minutes. You might look at the magic-command `%time`: sometimes easier to use than setting up start and end time, e.g., `%time some_function()` will give you the elapsed time for getting a return value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 1\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Build the comat using sentences from gothic_table with a spread of 1, i.e., 1 word on either side of target word. Please use sorted labels on your comat. This is different than the sample comat above which uses the order the words are seen. So your matrix should have these as the first 10 rows and columns.\n",
    "<p>\n",
    "<pre>\n",
    "'aaem'\n",
    "'ab'\n",
    "'aback'\n",
    "'abaft'\n",
    "'abandon\n",
    "'abandoned'\n",
    "'abandoning'\n",
    "'abandonment'\n",
    "'abaout'\n",
    "'abased'\n",
    "</pre>\n",
    "<p>\n",
    "Use any raw Python data structures you like. I am also ok with a pandas dataframe if you want.\n",
    "<p>\n",
    "I've given you my results to match against below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gothic_table = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQqRwyE0ceZREKqhuaOw8uQguTG6Alr5kocggvAnczrWaimXE8ncR--GC0o_PyVDlb-R6Z60v-XaWm9/pub?output=csv',\n",
    "                          encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gothic_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19579"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gothic_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Go to it\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Add your own code cells to create and test your code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "swords = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "import string\n",
    "punctuation = string.punctuation\n",
    "\n",
    "#legals = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def sentence_wrangler(sentence, swords, punctuation):\n",
    "    tokes = word_punct_tokenizer.tokenize(sentence)\n",
    "    result = []\n",
    "    removed = []\n",
    "    check = 0\n",
    "    for word in tokes:\n",
    "        if word.lower() not in swords:\n",
    "            if word in punctuation:\n",
    "                removed.append(word)\n",
    "            else:\n",
    "                for i in word:\n",
    "                    if i in punctuation:\n",
    "                        check = 0\n",
    "                    else:\n",
    "                        check = 1\n",
    "                if check == 1:\n",
    "                    result.append(word.lower())\n",
    "                else:\n",
    "                    removed.append(word.lower())\n",
    "        else:\n",
    "            removed.append(word.lower())\n",
    "    return (result, removed)\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    AB = 0.0\n",
    "    A = 0.0\n",
    "    B = 0.0\n",
    "    for i in range(len(v1)):\n",
    "        AB += (v1[i] * v2[i])\n",
    "        A += (v1[i]**2)\n",
    "        B += (v2[i]**2)  \n",
    "    return AB/((A*B)**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is a starting cell - add new ones with + tab above.\n",
    "def comat_maker(k, table, stop_words, punctuation):\n",
    "    all_words = {}\n",
    "    manifest = {}\n",
    "    for i in range(len(table)):\n",
    "        new = {}\n",
    "        vec = table.loc[i, 'text']\n",
    "        sentence = sentence_wrangler(vec, stop_words, punctuation)[0]\n",
    "        for j in range(len(sentence)):\n",
    "            if sentence[j] not in all_words:\n",
    "                manifest[sentence[j]] = 0\n",
    "                if j < len(sentence)-k and j == 0:\n",
    "                    new = {}\n",
    "                    for z in range(k):\n",
    "                        new[sentence[j+(z+1)]] = 1\n",
    "                    all_words[sentence[j]] = new\n",
    "                elif j < len(sentence)-k:\n",
    "                    new = {}\n",
    "                    for z in range(k):\n",
    "                        new[sentence[j-(z+1)]] = 1\n",
    "                    for z in range(k):\n",
    "                        new[sentence[j+(z+1)]] = 1\n",
    "                    all_words[sentence[j]] = new\n",
    "                elif j == len(sentence)-k:\n",
    "                    new = {}\n",
    "                    for z in range(k):\n",
    "                        new[sentence[j-(z+1)]] = 1\n",
    "                    all_words[sentence[j]] = new\n",
    "            else:\n",
    "                if j < len(sentence)-k and j == 0:\n",
    "                    for z in range(k):\n",
    "                        if sentence[j+(z+1)] in all_words[sentence[j]]:\n",
    "                            all_words[sentence[j]][sentence[j+(z+1)]] += 1\n",
    "                        else:\n",
    "                            all_words[sentence[j]][sentence[j+(z+1)]] = 1\n",
    "                elif j < len(sentence)-k:\n",
    "                    for z in range(k):\n",
    "                        if sentence[j-(z+1)] in all_words[sentence[j]]:\n",
    "                            all_words[sentence[j]][sentence[j-(z+1)]] += 1\n",
    "                        else:\n",
    "                            all_words[sentence[j]][sentence[j-(z+1)]] = 1     \n",
    "                        if sentence[j+(z+1)] in all_words[sentence[j]]:\n",
    "                            all_words[sentence[j]][sentence[j+(z+1)]] += 1\n",
    "                        else:\n",
    "                            all_words[sentence[j]][sentence[j+(z+1)]] = 1\n",
    "                elif j == len(sentence)-k:\n",
    "                    for z in range(k):\n",
    "                        \n",
    "                        if sentence[j-(z+1)] in all_words[sentence[j]]:\n",
    "                            all_words[sentence[j]][sentence[j-(z+1)]] += 1\n",
    "                        else:\n",
    "                            all_words[sentence[j]][sentence[j-(z+1)]] = 1 \n",
    "                        \n",
    "    return (all_words, manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Match against my results\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I don't know how you are implementing the comat so I'll just describe the results I got below. Show me that you get the same.\n",
    "\n",
    "<ul>\n",
    "<li>The count of 0s in row 'monster' is 24765.\n",
    "<li>The cosine_similarity between the rows 'frankenstein' and 'monster' is 0.07273929674533079\n",
    "<li>The sum of the 'frankenstein' row is 32.\n",
    "<li>The sum of the 'monster' row is 74.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24875\n",
      "69\n",
      "0.0727392967453\n",
      "32\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "#show me your match\n",
    "info = comat_maker(1, gothic_table, swords, punctuation)\n",
    "\n",
    "print (len(info[1]) - len(info[0]['monster']))\n",
    "print len(info[0]['monster'])\n",
    "\n",
    "def build_vectors(dict1, dict2):\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    #match dictionary size\n",
    "    for word in sorted(dict1):\n",
    "        if word not in dict2:\n",
    "            dict2[word] = 0\n",
    "    for word in sorted(dict2):\n",
    "        if word not in dict1:\n",
    "            dict1[word] = 0\n",
    "    #create vectors out of values\n",
    "    for word in sorted(dict1):\n",
    "        d1.append(dict1[word])\n",
    "    for word in sorted(dict2):\n",
    "        d2.append(dict2[word])\n",
    "    return (d1, d2)\n",
    "\n",
    "vecs = build_vectors(info[0]['monster'], info[0]['frankenstein'])\n",
    "\n",
    "frank_monster = cosine_similarity(vecs[0], vecs[1])\n",
    "print frank_monster\n",
    "\n",
    "total_frank = 0\n",
    "for count in info[0]['frankenstein'].values():\n",
    "    total_frank += count\n",
    "print total_frank\n",
    "\n",
    "total_monster = 0\n",
    "for count in info[0]['monster'].values():\n",
    "    total_monster += count\n",
    "print total_monster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 2\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Do the first part of KNN. Find all the distances from a specific word as a row in comat. Use cosine_similarity. Match my results.\n",
    "<p>\n",
    "BTW: this is where things slowed down and I was getting 9 minutes on my slow machine.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [u'uncomplainingly', u'hernani', u'skulking',u'mimes', u'decipherer', u'ever', u'thought', u'sentience', u'never', u'indistinctly']\n",
    "test2 = [u'unrelenting', u'tempora', u'madam', u'fie', u'moses', u'victor', u'cousin', u'margaret', u'demoniac', u'fallacious']\n",
    "def word_distances(matrix, word):\n",
    "    all_distances = []\n",
    "    for other_word in test1:\n",
    "        vecs = build_vectors(matrix[0][word], matrix[0][other_word])\n",
    "        distance = cosine_similarity(vecs[0], vecs[1])\n",
    "        all_distances.append((other_word, distance))\n",
    "    sorted_distances = sorted(all_distances, key=lambda pair: pair[1], reverse=True)\n",
    "    return sorted_distances\n",
    "    \n",
    "matrix = comat_maker(1, gothic_table, swords, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.1 ms, sys: 4.05 ms, total: 29.2 ms\n",
      "Wall time: 26.1 ms\n"
     ]
    }
   ],
   "source": [
    "%time monster_distances = word_distances(matrix, 'monster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'uncomplainingly', 0.23145502494313785),\n",
       " (u'hernani', 0.2182178902359924),\n",
       " (u'skulking', 0.2182178902359924),\n",
       " (u'mimes', 0.2182178902359924),\n",
       " (u'decipherer', 0.2182178902359924),\n",
       " (u'ever', 0.19495617958510608),\n",
       " (u'thought', 0.19014352249619199),\n",
       " (u'sentience', 0.17251638983558856),\n",
       " (u'never', 0.1697162329953409),\n",
       " (u'indistinctly', 0.1690308509457033)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monster_distances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 ms, sys: 621 µs, total: 3.19 ms\n",
      "Wall time: 2.64 ms\n"
     ]
    }
   ],
   "source": [
    "%time frank_distances = word_distances(matrix, 'frankenstein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'unrelenting', 0.3333333333333333),\n",
       " (u'tempora', 0.3333333333333333),\n",
       " (u'madam', 0.31277162108561213),\n",
       " (u'fie', 0.2721655269759087),\n",
       " (u'moses', 0.2721655269759087),\n",
       " (u'victor', 0.26653757937535255),\n",
       " (u'cousin', 0.26470046277199466),\n",
       " (u'margaret', 0.2545875386086578),\n",
       " (u'demoniac', 0.23570226039551587),\n",
       " (u'fallacious', 0.23570226039551587)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frank_distances[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Closing notes\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "If we had more time in the quarter, I'd like to follow up on the next step we can take once we have the comax. The general idea is that the comax is a very sparse matrix, consisting of mostly 0 entries. And as you have seen, it takes time to search it. There are linear-algebra  techniques for reducing a sparse matrix into a more dense matrix and hence, making them more computationally tractable to work with. One I particular wish we had time to explore is Singular-value Decomposition (or SVD): https://en.wikipedia.org/wiki/Singular-value_decomposition. This is used with the Glove algorithm to take a comax, exactly like the one you built, and reduce it: https://nlp.stanford.edu/projects/glove/.\n",
    "<p>\n",
    "An alternative approach is one called word-to-vec (or word2vec). This uses a shallow neural-net to reduce each word to a vector of \"features\". Here is a good overview: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/. You may have seen some of the cool things they can do with these feature vectors, e.g., take the vector for 'king', subtract the vector for 'man' and add the vector for 'woman. The resulting vector is 'queen'.\n",
    "<p>\n",
    "If I was going to teach a follow-on course to this one, I would definitely want to follow the neural-net and deep-learning path further. If anyone is interested in doing a reading (for credit) on these topics in summer or next year, see me.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
